{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Saurav's writings","text":""},{"location":"Bayesian_Learning_Part1/","title":"Bayesian Learning: Part I","text":"<p>In this post I want to lay out some of the basic concepts of Bayesian inference as used in the machine learning context. The content on this post is heavily derived from [1, 2]. This post and my interes in Bayesian inference has been inspired by talks I attend with the Boston Bayesian Meetup group.</p> <p>In supervised machine learning, we wish to learn relation between two variables \\(A\\) and \\(B\\) given some sample examples. Once the relation has been 'learned', the we can predict 'what is \\(B\\)' given \\(A\\). In practice the measurement of both the dependent (\\(B\\)) and independent (\\(A\\)) variable is inexact and the relation between them is not deterministic. Given this inherent uncertainty it does not make sense to have a deterministic for \\(B\\) with complete certainty. A probabilistic approach is better suited for meaningful reasoning in presence of uncertainty.</p> <p>The probabilistic approach treats both \\(A\\) and \\(B\\) as random variables and relates the two via the conditional probability \\(P(B|A)\\) i.e., the probability of \\(B\\) given a specific value of \\(A\\). The first step in machine learning is to approximate the likelihood \\(P(B|A)\\) with an appropriate model based on a given set of examples of \\(A\\) and \\(B\\). Typically, the conditional probability \\(P(B|A)\\) has a parameterized model  $$ \\newcommand{\\weight}{\\mathbf{w}} \\newcommand{\\dataset}{\\mathcal{D}} \\newcommand{\\paramvec}{\\mathbf{\\theta}} \\newcommand{\\xvec}{\\mathbf{x}} \\newcommand{\\xmat}{\\mathbf{X}} \\newcommand{\\yvec}{\\mathbf{y}} \\newcommand{\\realnum}{\\mathbb{R}} \\newcommand{eye}{\\mathbf{I}} \\newcommand{\\cov}{\\mathbf{V}} \\newcommand{\\inv}{{-1}} P(B|A) = f(A;\\weight), $$ where \\(\\weight\\) is a vector of model parameters. Then given a set \\(\\dataset\\) of \\(N\\) examples \\(\\dataset = \\{A_n, B_n\\}_{n=1:N}\\), a conventional approach involves optimization (e.g. maximum likelihood, least-squares fit) of our model for the given data \\(\\dataset\\) w.r.t. the parameters \\(\\weight\\). With the optimal parameters \\(\\weight_{opt}\\) computed, we can predict the unknown \\(B\\) by evaluating \\(f(A_*; \\weight_{opt})\\) given \\(A_*\\). </p> <p>The Bayesian inference approach treats the model parameters \\(\\weight\\) as random variables. The conditional probability of \\(B\\) now depends on both \\(A\\) and \\(\\weight\\): \\(P(B|A, \\weight)\\). Rather than 'learning' specific values for \\(\\weight\\), we can infer a posterior distribution \\(P(\\weight|A, B)\\) over the parameters \\(\\weight\\) using the Bayes' rule. </p> <p>In general, the Bayesian approach for learning model parameters involves following steps: * Given \\(n\\) data, \\(\\dataset\\), write down the likelihood expression \\(P(B|, \\weight)\\). * Specify a prior: \\(p(\\weight)\\) * Compute the posterior $$ p(\\weight|B) = \\frac{p(B|\\weight)p(\\weight)}{p(B)}.$$</p>"},{"location":"Bayesian_Learning_Part1/#linear-prediction","title":"Linear Prediction","text":"<p>Lets look at an example of the classic linear regression problem. A training dataset \\(\\dataset\\) with \\(N\\) examples \\(\\{\\xvec_{1:N}, y_{1:N}\\}\\) is given. Each input \\(\\xvec_i \\in \\realnum^M\\) is a vector with \\(M\\) attributes and each output \\(y_i \\in \\realnum\\) is considered univariate. In linear prediction, the relation between input and output in the training dataset is approximated by a parameterized linear model \\(y(\\xvec, \\paramvec)\\) where parameter vector \\(\\paramvec = \\{\\theta_1, \\theta_2,\\ldots,\\theta_M\\}\\). The linear model is expressed as: $$ y_i = \\sum_{j=1}^M x_{ij}\\theta_j,  $$ or a more compact matrix form is $$ \\yvec = \\xmat\\paramvec, $$ where \\(\\yvec\\) is an \\(N\\times1\\) vector whose entries are \\(N\\) outputs of training data and \\(\\xmat\\) is an \\(N\\times M\\) matrix whose rows are the input vectors of the training dataset. The problem is to estimate the parameters \\(\\paramvec\\) such that our linear model gives the 'best' fit to the provided training dataset \\(\\dataset\\). The estimated parameter \\(\\hat{\\theta}\\) can be used to predict the ouput for input values not in the training dataset.</p>"},{"location":"Bayesian_Learning_Part1/#least-squares","title":"Least squares","text":"<p>The least squares (LS) estimate of the parameter. $$ \\hat{\\paramvec} = (\\xmat^T\\xmat)^{-1}\\xmat^T\\yvec $$</p>"},{"location":"Bayesian_Learning_Part1/#maximum-likelihood","title":"Maximum likelihood","text":"<p>The maximum likelihood (ML) estimate of the parameters \\(\\paramvec\\) is same as the LS estimate. Hence the LS parameter estimate is optimal in ML sense.</p>"},{"location":"Bayesian_Learning_Part1/#bayesian-linear-regression","title":"Bayesian linear regression","text":"<p>Posterior mean: \\(\\paramvec = (\\lambda\\eye_M + \\xmat^T\\xmat)^{\\inv}\\xmat^T\\yvec\\) Posterior variance: \\(\\cov_N = \\sigma^2(\\lambda\\eye_M + \\xvec^T\\xvec)^{\\inv}\\) where \\(\\lambda\\) is a constant (to be defined).</p>"},{"location":"Bayesian_Learning_Part1/#references","title":"References","text":"<ol> <li>Bayesian Inference: An Introduction to Principles and Practice in Machine Learning, Michael E. Tipping, Microsoft Research, Cambridge, U.K. (http://www.miketipping.com/papers.htm)   </li> <li>Machine Learning (CPSC 540) lecture notes from Prof. Nando de Freitas, UBC (http://www.cs.ubc.ca/~nando/540-2013/index.html)</li> </ol>"},{"location":"about/","title":"About Saurav","text":"<p>Saurav Tuladhar</p>"},{"location":"beamforming_fft/","title":"Beamforming and FFT","text":"<p>This notebook explains the relation between spatial beamforming and the time domain FFT operation and show how beamforming can be implemented using FFT. The content presented below is loosely based on a tutorial by Prof. John R. Buck at UMassD.</p> <p>This document is presented as an IPython notebook. </p> <p>This video tutorial demonstrates how to setup Anaconda package for Python on a Windows machine. Similar approach is applicable for Linux and OSX platforms.</p> <p></p>"},{"location":"beamforming_fft/#discrete-fourier-transform-dft","title":"Discrete Fourier Transform (DFT)","text":"<p>Given a finite length discrete time sequence \\(x[m]\\) for \\(0 \\leq m \\leq M - 1\\), the discrete time fourier transform (DTFT) is defined as \\(\\(X(e^{j\\omega}) = \\sum\\limits_{m=0}^{M-1} x[n] e^{-jm\\omega}.\\)\\) \\(X(e^{j\\omega})\\) is a continuous function of the radial frequency \\(\\omega\\) and \\(-\\pi \\leq \\omega \\leq \\pi\\). </p> <p>The discrete fourier transform (DFT) can be viewed as the fequency domain sampling of the DTFT every \\(\\Delta\\omega = 2\\pi/N\\). Time domain sampling results in creating copies of the spectrum in the frequency domain. Similarly, sampling in the frequency domain has an effect of creatign copies of the time domain sequence at every \\(N\\). Hence to avoid aliasing in time, we need to choose \\(N \\geq M\\) to avoid aliasing in time. The DFT is defined as</p> \\[ X[l] = X(e^{j\\omega})\\vert_{\\omega = 2\\pi l/N} \\] \\[X[l] = \\sum\\limits_{n=0}^{N-1} x[n] e^{-j(2\\pi/N)ln} \\quad l = 0,\\ldots, N-1\\] <p>Above computation requries \\(\\mathcal{O}(N^2)\\).</p> <p>Fast Fourier Trasform is a divide-and-conquer based approach to evaluate the DFT with fewer number of operation, specifically \\(\\mathcal{O}(N\\log_2 N)\\).</p>"},{"location":"beamforming_fft/#time-domain-filtering","title":"Time domain filtering","text":"<p>DTFT of impuse response \\(h[n]\\) of the system</p> <p>Frequency response: \\(H(e^{j\\omega}) = \\sum\\limits_{m=0}^{M-1}h[m]e^{-j\\omega m}\\)</p> <p>\\(X_N[l] = DFT(x[m])\\)</p> <p>Analogy:finite length signals = finite aperture </p>"},{"location":"beamforming_fft/#spatial-domain","title":"Spatial domain","text":"<p>In spatial domain, signals are considered to be a finite set of narrowband signals observed using a uniform line array (ULA) with \\(N\\) sensors. The measured signal is represented as a vector array data \\(\\mathbf{x}\\).</p> <p>A unit amplitude planewave signal arriving from direction \\(\\theta\\) is modeled using the array manifold vector/replica vector \\(\\mathbf{v}(\\theta)\\). The replica vector is a vector of complex exponentials, $$ \\begin{align} [\\mathbf{v}(\\theta)]_n  =&amp; [e^{j(2\\pi d/\\lambda)n\\cos(\\theta)}] \\  =&amp; [e^{j(2\\pi d/\\lambda)nu}] \\end{align} $$ where \\(0 \\leq \\theta \\leq \\pi\\) is the source bearing direction and the direction cosine is \\(u = \\cos(\\theta)\\). The wavenumber is defined as \\(k = 2\\pi/\\lambda\\).</p> <p>Beampattern defines the gain due to beamformer for each planewave from different direction \\(B(u) = \\mathbf{w}^{H}\\mathbf{v}(u), \\quad \\text{for} -1\\leq u \\leq 1\\). Beampattern is analogous to the frequency response \\(H(e^{j\\omega}\\) of a DT filter described above.</p> \\[ B(u) = \\sum w^* e^{-jkdun}\\] <p>\\(\\omega = kdu\\) and \\(\\Delta\\omega = kd\\Delta u = 2\\pi/N\\)</p> <p>Scanned response defines the spatial spectrum of a propagating signal. The scanned response is analogous to the frequency spectrum of a DT signal. \\(\\(y = \\mathbf{w}^{H}(u)\\mathbf{x}\\)\\)</p> \\[\\mathbf{w}(u) = \\frac{1}{N}[e^{-jkdun}]$$ $$y(u) = \\frac{1}{N}\\sum\\limits x_n e^{jkdun}\\] <p>fft(conj(w), N)</p> <p>If \\(d = \\lambda/2\\) so that \\(kd = \\pi\\) then \\(\\Delta \\omega = \\pi \\Delta u\\)</p> <p>Side note * Need to be careful when \\(d \\neq \\lambda/2\\), the FFT output will have invisible areas too. * scanned response can be implemented as IFFT * Test with non-symmetric scenario * FFT is memory efficient, 'in place'  * Creating exponential vectors will be memory intensive</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.signal as signal\nfrom scipy.fftpack import fft, fftshift, ifft\nfrom numpy.random import randn\n% matplotlib inline\n</code></pre> <pre><code>from matplotlib import rc\nrc('text', usetex = True)\nrc('font', family = 'serif')\n</code></pre> <p>** Function defines the steering/replica vector**</p> <p>\\([\\mathbf{v}] = [e^{-j2\\pi d/\\lambda n \\cos(\\theta)}]\\)</p> <pre><code>def steering_vector(N, u, beta):\n    # beta: sampling ratio, 1 =&gt; Nyquist sampling\n    n = np.arange(0, N)\n    sv = np.exp(-1j*np.pi*beta*u*n)\n    return sv    \n</code></pre> <pre><code>N = 11\nsv = steering_vector(N, np.array(0), 1)\nNfft = 1024\n</code></pre> <p>Direct implementation</p> <pre><code>uscan = np.linspace(-1, 1, Nfft)\nV = np.empty((11, Nfft), dtype=complex)\nidx = 0\nfor u in uscan:\n    V[:, idx] = steering_vector(11, u, 1)\n    idx = idx + 1\n\nBP = np.dot(sv/N, V)\n</code></pre> <p>FFT based implementation</p> <pre><code>u = np.linspace(-(Nfft-1)/2, Nfft/2, Nfft)*2/Nfft\nBP_fft = (1/N)*fftshift(fft(sv, Nfft))\n</code></pre> <pre><code>f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.plot(u, 20*np.log10(np.abs(BP_fft)), lw=2)\nax1.set_title('FFT Beampattern')\nax2.plot(uscan, 20*np.log10(np.abs(BP)), lw=2)\nax2.set_title(r'u-scan Beampattern')     \nax1.set_xlabel(r'u = \\cos(\\theta)')\nax1.grid(True)\nax2.grid(True)\n#ax1.set_xl\n</code></pre> <p></p>"},{"location":"beamforming_fft/#scan-response-example","title":"Scan response example","text":"<pre><code>u1 = 3/11\nu2 = -5/11\nsv1 = steering_vector(N, u1, 1)\nsv2 = steering_vector(N, u2, 1)\nx = sv1 + 0.5*sv2\nX = (Nfft/N)*fftshift(ifft(x, Nfft))\n\nplt.figure()\nplt.plot(u, 20*np.log10(np.abs(X)), lw=2)\nplt.ylim((-80, 0))\nplt.axvline(x=u1, color='k', linestyle='--', alpha=0.5)\nplt.axvline(x=u2, color='k', linestyle='--', alpha=0.5)\nplt.grid(True)\nplt.xlabel(r'u = \\cos( \\theta )')\nplt.ylabel(r'Scan response dB')\n#plt.annotate('u_1', xy=(u1, 0.1), xytext=(u1, 0.2))\n#plt.annotate('u_2', xy=(u2, 0.1), xytext=(u2, 0.2))\n#plt.savefig(filename='two_source_response.pdf', dpi=120)\n</code></pre> <pre><code>&lt;matplotlib.text.Text at 0x10a9566a0&gt;\n</code></pre>"}]}